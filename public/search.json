[
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/01_fastqc.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/01_fastqc.html",
    "title": "Run FastQC on sample",
    "section": "",
    "text": "We can start by checking some QC metrics of the fasta files.\nTo count the number of fastq files we can use the following:\n\nCodels -lR /gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_2/ | grep --count \\.fastq.gz$ \n\n\nThere’s 144 in set 1, 244 in set 2, and 656 in set 3.\nIt’s a little graceless, but I’ll purl this script for set 1 and then manually create copies and change the array number and directory.\n\nCode#!/bin/bash\n\n#SBATCH -p c_highmem_dri1\n#SBATCH --job-name=fastqc_bbb\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --array=1-144%144\n#SBATCH --mem-per-cpu=5000 # memory limit per core\n#####  #SBATCH --mem=96000 # memory limit per compute node for the job\n#SBATCH --time=1-00:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%A_%a_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%A_%a_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"All jobs in this array have:\"\necho \"- SLURM_ARRAY_JOB_ID: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_COUNT: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM_ARRAY_TASK_MIN: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM_ARRAY_TASK_MAX: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM_JOB_ID: ${SLURM_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_ID: ${SLURM_ARRAY_TASK_ID}\"\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n## FASTQ files\n## This if for batch 1 set 1\nINPUT_DIR_b1s1=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_1/211008_A00748_0157_AHT7TJDSX2_fastq_L2_3_4/\"\n## This is for batch 1 set 2\nINPUT_DIR_b1s2=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_2/\"\n## This is the directory for batch 2 (set 3)\nINPUT_DIR=\"/gluster/dri02/rdsmbh/shared/rdsmbh/230327_A00748_0368_AH5CTMDSX5_fastq/\"\n\nOUTPUT_DIR=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/01_fastqc/01_set1\"\nOUTPUT_DIR2=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/01_fastqc/02_set2\"\nOUTPUT_DIR3=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/01_fastqc/03_set3\"\n\n## Load FastQC module\nmodule load FastQC\n\n#-----------------------------------------------------------------------\n\nmkdir -p $OUTPUT_DIR\n\nN=${SLURM_ARRAY_TASK_ID}\n\nFASTQ_FILE=$(find $INPUT_DIR_b1s1 -mindepth 1 -maxdepth 1 -name '*_001.fastq.gz' | sort | tail -n+${N} | head -1)\nfastqc -o $OUTPUT_DIR1 -f fastq --noextract --quiet -t 1 $FASTQ_FILE\n \n# FASTQ_FILE=$(find $INPUT_DIR_b1s2 -mindepth 1 -maxdepth 3 -name '*_001.fastq.gz' | sort | tail -n+${N} | head -1)\n# fastqc -o $OUTPUT_DIR2 -f fastq --noextract --quiet -t 1 $FASTQ_FILE\n\n# FASTQ_FILE=$(find $INPUT_DIR -mindepth 1 -maxdepth 1 -name '*_001.fastq.gz' | sort | tail -n+${N} | head -1)\n# fastqc -o $OUTPUT_DIR3 -f fastq --noextract --quiet -t 1 $FASTQ_FILE\n\n\necho -e \"\\n*****************************************************************\"\necho \"Finished at: \"`date`\necho \"*****************************************************************\""
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/02_multiqc.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/02_multiqc.html",
    "title": "Run MultiQC",
    "section": "",
    "text": "MultiQC combines the fastqc reports together for us.\nYou can view the report for set 1 reads here, set 2 here and set 3 here.\nNote that the files containing a string like _I1_ are the indexes while strings like _R1_ are the reads. There’s a separate MultiQC report for each set and for the indexes and reads.\n\nCode#!/bin/bash\n\n#SBATCH -p c_highmem_dri1\n#SBATCH --job-name=multiqc_bbb\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=1-00:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%A_%a_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%A_%a_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"All jobs in this array have:\"\necho \"- SLURM_ARRAY_JOB_ID: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_COUNT: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM_ARRAY_TASK_MIN: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM_ARRAY_TASK_MAX: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM_JOB_ID: ${SLURM_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_ID: ${SLURM_ARRAY_TASK_ID}\"\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n# Load module\nmodule load multiqc\n\n## collect MultiQC reports\nOUTPUT_DIR=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/02_multiqc/\"\n\n#-----------------------------------------------------------------------\n# FastQC runs\n\n## Endo 10X Vascular and Parenchymal fraction set 1 - 24 samples\nINPUT_DIR=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/01_fastqc/01_set1/\"\n## Endo 10X Vascular and Parenchymal fraction set 2 - 16 samples\nINPUT_DIR2=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/01_fastqc/02_set2/\"\n## Endo 10X Vascular and Parenchymal fraction set 3 - 40 samples\nINPUT_DIR3=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/01_fastqc/03_set3/\"\n\nmultiqc -o $OUTPUT_DIR\"01_set1_reads\" \\\n--filename \"fastqc_endo_10X_set1_reads\" \\\n--title \"FastQC endo 10X set1 reads\" \\\n--cl_config \"fastqc_config: { fastqc_theoretical_gc: hg38_txome }\" \\\n--module fastqc \\\n--force \\\n--interactive \\\n$INPUT_DIR\"\"*_R[1-2]_*_fastqc.zip\n\nmultiqc -o $OUTPUT_DIR\"01_set1_index\" \\\n--filename \"fastqc_endo_10X_set1_index\" \\\n--title \"FastQC endo 10X set1 index\" \\\n--cl_config \"fastqc_config: { fastqc_theoretical_gc: hg38_txome }\" \\\n--module fastqc \\\n--force \\\n--interactive \\\n$INPUT_DIR\"\"*_I[1-2]_*_fastqc.zip\n\nmultiqc -o $OUTPUT_DIR\"02_set2_reads\" \\\n--filename \"fastqc_endo_10X_set2_reads\" \\\n--title \"FastQC endo 10X set2 reads\" \\\n--cl_config \"fastqc_config: { fastqc_theoretical_gc: hg38_txome }\" \\\n--module fastqc \\\n--force \\\n--interactive \\\n$INPUT_DIR2\"\"*_R[1-2]_*_fastqc.zip\n\nmultiqc -o $OUTPUT_DIR\"02_set2_index\" \\\n--filename \"fastqc_endo_10X_set2_index\" \\\n--title \"FastQC endo 10X set2 index\" \\\n--cl_config \"fastqc_config: { fastqc_theoretical_gc: hg38_txome }\" \\\n--module fastqc \\\n--force \\\n--interactive \\\n$INPUT_DIR2\"\"*_I[1-2]_*_fastqc.zip\n\nmultiqc -o $OUTPUT_DIR\"03_set3_reads\" \\\n--filename \"fastqc_endo_10X_set3_reads\" \\\n--title \"FastQC endo 10X set3 reads\" \\\n--cl_config \"fastqc_config: { fastqc_theoretical_gc: hg38_txome }\" \\\n--module fastqc \\\n--force \\\n--interactive \\\n$INPUT_DIR3\"\"*_R[1-2]_*_fastqc.zip\n\nmultiqc -o $OUTPUT_DIR\"03_set3_index\" \\\n--filename \"fastqc_endo_10X_set3_index\" \\\n--title \"FastQC endo 10X set3 index\" \\\n--cl_config \"fastqc_config: { fastqc_theoretical_gc: hg38_txome }\" \\\n--module fastqc \\\n--force \\\n--interactive \\\n$INPUT_DIR3\"\"*_I[1-2]_*_fastqc.zip\n\necho -e \"\\n*****************************************************************\"\necho \"Finished at: \"`date`\necho \"*****************************************************************\""
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/03_generate_cellranger_reference.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/03_generate_cellranger_reference.html",
    "title": "Generate cellranger reference",
    "section": "",
    "text": "In order to run the cellranger pipeline one needs a reference genome for it to align to. 10X do provide a human reference, but it’s rather old at time of writing, so here we’ll generate a more recent one.\nNote that the initial download of the reference files are to be done on the login node. I’ll save them in the shared Webber dir on gluster (/gluster/dri02/rdscw/shared/webber/reference_genomes) in case others want to use it.\n\nCode#!/bin/bash\n\n#SBATCH -p c_highmem_dri1   ###c_highmem_dri1 ## dev, compute, htc, highmem\n#SBATCH --job-name=cellranger_ref_gen_bbb\n#SBATCH --ntasks=40\n#SBATCH --ntasks-per-node=40\n#### SBATCH --mem-per-cpu=30000 # memory limit per core\n#SBATCH --mem=50G # memory limit per compute node for the job\n#SBATCH --time=3-00:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"job ID: \"$SLURM_JOBID\necho \"job name: \"$SLURM_JOB_NAME\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n#-----------------------------------------------------------------------\n\n## code from Cell Ranger (CR) website\n## modified to retrieve GRCh38 genome build from specific ensembl release\n## the default CR reference from July 2020 (refdata-gex-GRCh38-2020-A.tar.gz) is based on ensembl v98/Gencode v32\n## https://support.10xgenomics.com/single-cell-gene-expression/software/release-notes/build\n##\n## NOTE: run first download part on login node\n## pre-processing and cellranger mkref on compute node\n\n#-----------------------------------------------------------------------\n\n## CR v7.1.0\nCELL_RANGER=\"/scratch/c.mpmgb/tools/cellranger-7.1.0/bin/cellranger\"\n\n## which ensembl and gencode version\nENSEMBL_VER=\"109\"\nGENCODE_VER=\"43\"\n\n## store reference data\nREF_DIR=\"/gluster/dri02/rdscw/shared/webber/reference_genomes\"\ncd $REF_DIR\n\n# Genome metadata\ngenome=\"GRCh38\"\nversion=\"2023_\"${ENSEMBL_VER}\n\n# Set up source and build directories\nbuild=\"GRCh38_2023_\"${ENSEMBL_VER}\"_ensembl\"\nmkdir -p \"$build\"\n\n# Download source files if they do not exist in reference_sources/ folder\nsource=\"reference_sources_ensembl_\"${ENSEMBL_VER}\nmkdir -p \"$source\"\n\nfasta_url=\"http://ftp.ensembl.org/pub/release-\"${ENSEMBL_VER}\"/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\"\nfasta_in=\"${source}/Homo_sapiens.GRCh38.dna.primary_assembly.fa\"\ngtf_url=\"http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_\"${GENCODE_VER}\"/gencode.v\"${GENCODE_VER}\".primary_assembly.annotation.gtf.gz\"\ngtf_in=\"${source}/gencode.v\"${GENCODE_VER}\".primary_assembly.annotation.gtf\"\n\n## compute node dir\nCOM_DIR=\"/scratch/c.mpmgb/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/03_cellranger_reference\"\n\n#-----------------------------------------------------------------------\n## run on login node\n\n# if [ ! -f \"$fasta_in\" ]; then\n#     curl -sS \"$fasta_url\" | zcat &gt; \"$fasta_in\"\n# fi\n# if [ ! -f \"$gtf_in\" ]; then\n#     curl -sS \"$gtf_url\" | zcat &gt; \"$gtf_in\"\n# fi\n\n#-----------------------------------------------------------------------\n\n## copy the downloaded reference \n# cp -a ${REF_DIR}\"/.\" $COM_DIR\n\n## run on compute node\n\n# Move to the scratch dir\ncd $COM_DIR\n\n# Modify sequence headers in the Ensembl FASTA to match the file\n# \"GRCh38.primary_assembly.genome.fa\" from GENCODE. Unplaced and unlocalized\n# sequences such as \"KI270728.1\" have the same names in both versions.\n#\n# Input FASTA:\n#   &gt;1 dna:chromosome chromosome:GRCh38:1:1:248956422:1 REF\n#\n# Output FASTA:\n#   &gt;chr1 1\nfasta_modified=\"$build/$(basename \"$fasta_in\").modified\"\n# sed commands:\n# 1. Replace metadata after space with original contig name, as in GENCODE\n# 2. Add \"chr\" to names of autosomes and sex chromosomes\n# 3. Handle the mitochrondrial chromosome\ncat \"$fasta_in\" \\\n    | sed -E 's/^&gt;(\\S+).*/&gt;\\1 \\1/' \\\n    | sed -E 's/^&gt;([0-9]+|[XY]) /&gt;chr\\1 /' \\\n    | sed -E 's/^&gt;MT /&gt;chrM /' \\\n    &gt; \"$fasta_modified\"\n\n\n# Remove version suffix from transcript, gene, and exon IDs in order to match\n# previous Cell Ranger reference packages\n#\n# Input GTF:\n#     ... gene_id \"ENSG00000223972.5\"; ...\n# Output GTF:\n#     ... gene_id \"ENSG00000223972\"; gene_version \"5\"; ...\ngtf_modified=\"$build/$(basename \"$gtf_in\").modified\"\n# Pattern matches Ensembl gene, transcript, and exon IDs for human or mouse:\nID=\"(ENS(MUS)?[GTE][0-9]+)\\.([0-9]+)\"\ncat \"$gtf_in\" \\\n    | sed -E 's/gene_id \"'\"$ID\"'\";/gene_id \"\\1\"; gene_version \"\\3\";/' \\\n    | sed -E 's/transcript_id \"'\"$ID\"'\";/transcript_id \"\\1\"; transcript_version \"\\3\";/' \\\n    | sed -E 's/exon_id \"'\"$ID\"'\";/exon_id \"\\1\"; exon_version \"\\3\";/' \\\n    &gt; \"$gtf_modified\"\n\n\n# Define string patterns for GTF tags\n# NOTES:\n# - Since GENCODE release 31/M22 (Ensembl 97), the \"lincRNA\" and \"antisense\"\n#   biotypes are part of a more generic \"lncRNA\" biotype.\n# - These filters are relevant only to GTF files from GENCODE. The GTFs from\n#   Ensembl release 98 have the following differences:\n#   - The names \"gene_biotype\" and \"transcript_biotype\" are used instead of\n#     \"gene_type\" and \"transcript_type\".\n#   - Readthrough transcripts are present but are not marked with the\n#     \"readthrough_transcript\" tag.\n#   - Only the X chromosome versions of genes in the pseudoautosomal regions\n#     are present, so there is no \"PAR\" tag.\nBIOTYPE_PATTERN=\\\n\"(protein_coding|lncRNA|\\\nIG_C_gene|IG_D_gene|IG_J_gene|IG_LV_gene|IG_V_gene|\\\nIG_V_pseudogene|IG_J_pseudogene|IG_C_pseudogene|\\\nTR_C_gene|TR_D_gene|TR_J_gene|TR_V_gene|\\\nTR_V_pseudogene|TR_J_pseudogene)\"\nGENE_PATTERN=\"gene_type \\\"${BIOTYPE_PATTERN}\\\"\"\nTX_PATTERN=\"transcript_type \\\"${BIOTYPE_PATTERN}\\\"\"\nREADTHROUGH_PATTERN=\"tag \\\"readthrough_transcript\\\"\"\nPAR_PATTERN=\"tag \\\"PAR\\\"\"\n\n\n# Construct the gene ID allowlist. We filter the list of all transcripts\n# based on these criteria:\n#   - allowable gene_type (biotype)\n#   - allowable transcript_type (biotype)\n#   - no \"PAR\" tag (only present for Y chromosome PAR)\n#   - no \"readthrough_transcript\" tag\n# We then collect the list of gene IDs that have at least one associated\n# transcript passing the filters.\ncat \"$gtf_modified\" \\\n    | awk '$3 == \"transcript\"' \\\n    | grep -E \"$GENE_PATTERN\" \\\n    | grep -E \"$TX_PATTERN\" \\\n    | grep -Ev \"$READTHROUGH_PATTERN\" \\\n    | grep -Ev \"$PAR_PATTERN\" \\\n    | sed -E 's/.*(gene_id \"[^\"]+\").*/\\1/' \\\n    | sort \\\n    | uniq \\\n    &gt; \"${build}/gene_allowlist\"\n\n\necho \"Filter GTF...\"\n\n# Filter the GTF file based on the gene allowlist\ngtf_filtered=\"${build}/$(basename \"$gtf_in\").filtered\"\n# Copy header lines beginning with \"#\"\ngrep -E \"^#\" \"$gtf_modified\" &gt; \"$gtf_filtered\"\n# Filter to the gene allowlist\ngrep -Ff \"${build}/gene_allowlist\" \"$gtf_modified\" \\\n    &gt;&gt; \"$gtf_filtered\"\n    \n    \necho \"Run Cell Ranger mkref...\"  \n\n## Create reference package\n$CELL_RANGER mkref \\\n--ref-version=\"$version\" \\\n--genome=\"$genome\" \\\n--fasta=\"$fasta_modified\" \\\n--genes=\"$gtf_filtered\" \\\n--memgb=250 \\\n--nthreads=40"
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/04_cellranger_count_set1.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/04_cellranger_count_set1.html",
    "title": "Generate cellranger reference for set 1",
    "section": "",
    "text": "Set 1 contains the first 12 donors, 24 samples (12x P - parenchymal and 12x V - vascular fraction), which are independently sequenced of Set_2 (the remaining 16 samples from 8 donors). The sequencing was performed for Set_1 initially in Oxford, but there were some issues with index hopping and cleaning the data did not seem to work too well. So Set_1 was re-sequenced in Cardiff and the folder is: 211008_A00748_0157_AHT7TJDSX2_fastq_L2_3_4\nFor Set_2 the data was sequenced initially on one Illumina lane, but then more lanes were added for better coverage. So the FASTQ files across the 4 subdirs need to be combined as input for one sample. I think there were a couple of corner cases for those samples, one sample was of lower quality and not sequenced again, and one extra run of another sample basically had just one read. I think the script to run CellRanger for Set_2 should mention this and deal with it.\nDue to this funk with the samples I’ll keep them are seperate scripts for all three sets.\n\nCode#!/bin/bash\n\n#SBATCH -p c_highmem_dri1 ## dev, compute, htc, highmem\n#SBATCH --job-name=cellranger_count_set1\n#SBATCH --ntasks=40\n#SBATCH --ntasks-per-node=40\n#SBATCH --array=1-24%2\n##### #SBATCH --mem-per-cpu=8000 # memory limit per core\n#SBATCH --mem=340G # memory limit per compute node for the job\n#SBATCH --time=3-00:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%A_%a_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%A_%a_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"All jobs in this array have:\"\necho \"- SLURM_ARRAY_JOB_ID: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_COUNT: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM_ARRAY_TASK_MIN: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM_ARRAY_TASK_MAX: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM_JOB_ID: ${SLURM_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_ID: ${SLURM_ARRAY_TASK_ID}\"\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n#-----------------------------------------------------------------------\n\n## FASTQ files, set 1, 24 samples, sequenced in Cardiff\n## NOTE, R2 length is 150bp, complete length will be used by default\n## alternatively, reads could be trimmed  with Cell Ranger option --r2-length=90\nINPUT_DIR=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_1/211008_A00748_0157_AHT7TJDSX2_fastq_L2_3_4/\"\n\n## results\nOUTPUT_DIR=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/04_cellranger_count/01_set1\"\n\n## original sample IDs, correpond to FASTQ file names\nSAMPLE_ID_FILE=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/90_sample_info/samples_set1_IDs.txt\"\n\n## corresponding new sample names to be set by --id\n## NOTE: some samples will be de-swapped, as they were wrongly labelled during library prep in Oxford\nSAMPLE_ID_NEW_FILE=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/90_sample_info/samples_set1_IDs_swap.txt\"\n\n## CR executable\nCELL_RANGER=\"/scratch/c.mpmgb/tools/cellranger-7.1.0/bin/cellranger\"\n\n## CR reference\n## pre-computed and downloaded from CR website as is (refdata-gex-GRCh38-2020-A.tar.gz)\n## based on ensembl v98/gencode v32\n## CR_REF=\"/scratch/c.mpmgb/Tools/CellRanger/CellRanger_references/refdata-gex-GRCh38-2020-A\"\n## updated CR reference based on ensembl v109 (Gencode v43)\n## from get_cellranger_reference.sh\nCR_REF=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/03_cellranger_reference/GRCh38\"\n\n#-----------------------------------------------------------------------\n# Cell Ranger count\n\nmkdir -p $OUTPUT_DIR\n\nN=${SLURM_ARRAY_TASK_ID}\n\nSAMPLE_ID=$(cat $SAMPLE_ID_FILE | tail -n+${N} | head -1)\nSAMPLE_ID_NEW=$(cat $SAMPLE_ID_NEW_FILE | tail -n+${N} | head -1)\n\necho \"Input dir: \"$INPUT_DIR\necho \"Sample: \"$SAMPLE_ID\necho \"Sample new: \"$SAMPLE_ID_NEW\n\ncd $OUTPUT_DIR\n\n$CELL_RANGER count --id=$SAMPLE_ID_NEW \\\n--fastqs=$INPUT_DIR \\\n--sample=$SAMPLE_ID \\\n--transcriptome=$CR_REF \\\n--localcores=40 \\\n--localmem=320 \\\n--include-introns=true \\\n--no-bam\n\n## optional:\n## R2 length is longer (150bp) than usually recommended/required (90bp)\n## --r2-length=90\n\necho -e \"\\n*****************************************************************\"\necho \"Finished at: \"`date`\necho \"*****************************************************************\""
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/05_cellranger_count_set2.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/05_cellranger_count_set2.html",
    "title": "Generate cellranger reference for set 2",
    "section": "",
    "text": "This is for set 2:\n\nCode#!/bin/bash\n\n#SBATCH -p c_highmem_dri1 ## dev, compute, htc, highmem\n#SBATCH --job-name=cellranger_count_set2\n#SBATCH --ntasks=40\n#SBATCH --ntasks-per-node=40\n#SBATCH --array=1-16%2\n##### #SBATCH --mem-per-cpu=8000 # memory limit per core\n#SBATCH --mem=340G # memory limit per compute node for the job\n#SBATCH --time=3-00:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%A_%a_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%A_%a_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"All jobs in this array have:\"\necho \"- SLURM_ARRAY_JOB_ID: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_COUNT: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM_ARRAY_TASK_MIN: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM_ARRAY_TASK_MAX: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM_JOB_ID: ${SLURM_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_ID: ${SLURM_ARRAY_TASK_ID}\"\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n\n## FASTQ files, set 2 with 16 samples, data from 4 runs, sequenced in Oxford\n## NOTE, V_19, only sequenced in 1 run (INPUT_DIR_1)\n## NOTE, V_20, 1 run (INPUT_DIR_2) only with a single read, skip that run\n## handled via if statement below\nINPUT_DIR_1=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_2/210707_A00711_0393_BHCLTKDSX2/FASTQ/\"\nINPUT_DIR_2=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_2/210721_A00711_0400_BHF2YWDSX2/FASTQ/\"\nINPUT_DIR_3=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_2/210728_A00711_0405_BHF5JLDSX2/FASTQ/\"\nINPUT_DIR_4=\"/gluster/dri02/rdscw/shared/webber/Endo_10X/FASTQ/Set_2/210901_A00711_0420_AHGJJLDSX2/FASTQ/\"\n\n## results\nOUTPUT_DIR=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/04_cellranger_count/02_set2\"\n\n## original sample IDs, correpond to FASTQ file names\nSAMPLE_ID_FILE=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/90_sample_info/samples_set2_4_runs.txt\"\n\n## corresponding new sample names to be set by --id\nSAMPLE_ID_NEW_FILE=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/90_sample_info/samples_set2_rename.txt\"\n\n## CR executable\nCELL_RANGER=\"/scratch/c.mpmgb/tools/cellranger-7.1.0/bin/cellranger\"\n\n## CR reference\n## pre-computed and downloaded from CR website as is (refdata-gex-GRCh38-2020-A.tar.gz)\n## based on ensembl v98/gencode v32\n## CR_REF=\"/scratch/c.mpmfw/Tools/CellRanger/CellRanger_references/refdata-gex-GRCh38-2020-A\"\n## updated CR reference based on ensembl v108 (Gencode v42)\n## from get_cellranger_reference.sh\nCR_REF=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/03_cellranger_reference/GRCh38\"\n\n\n#-----------------------------------------------------------------------\n# Cell Ranger count\n\nmkdir -p $OUTPUT_DIR\n\nN=${SLURM_ARRAY_TASK_ID}\n\nSAMPLE_ID=$(cat $SAMPLE_ID_FILE | tail -n+${N} | head -1)\nSAMPLE_ID_NEW=$(cat $SAMPLE_ID_NEW_FILE | tail -n+${N} | head -1)\n\necho \"Input dir(s): \"$INPUT_DIR_1\", \"$INPUT_DIR_2\", \"$INPUT_DIR_3\", \"$INPUT_DIR_4\necho \"Sample: \"$SAMPLE_ID\necho \"Sample new: \"$SAMPLE_ID_NEW\n\ncd $OUTPUT_DIR\n\nif [[ \"$SAMPLE_ID_NEW\" == \"V_19\" ]]\nthen\n    $CELL_RANGER count --id=$SAMPLE_ID_NEW \\\n    --fastqs=$INPUT_DIR_1 \\\n    --sample=$SAMPLE_ID \\\n    --transcriptome=$CR_REF \\\n    --localcores=40 \\\n    --localmem=330 \\\n    --include-introns=true \\\n    --no-bam\nelif [[ \"$SAMPLE_ID_NEW\" == \"V_20\" ]]\nthen\n    $CELL_RANGER count --id=$SAMPLE_ID_NEW \\\n    --fastqs=$INPUT_DIR_1,$INPUT_DIR_3,$INPUT_DIR_4 \\\n    --sample=$SAMPLE_ID \\\n    --transcriptome=$CR_REF \\\n    --localcores=40 \\\n    --localmem=330 \\\n    --include-introns=true \\\n    --no-bam\nelse\n    $CELL_RANGER count --id=$SAMPLE_ID_NEW \\\n    --fastqs=$INPUT_DIR_1,$INPUT_DIR_2,$INPUT_DIR_3,$INPUT_DIR_4 \\\n    --sample=$SAMPLE_ID \\\n    --transcriptome=$CR_REF \\\n    --localcores=40 \\\n    --localmem=330 \\\n    --include-introns=true \\\n    --no-bam\nfi\n\n## optional:\n## R2 length is longer (151bp) than usually recommended/required (90bp)\n## --r2-length=90\n\necho -e \"\\n*****************************************************************\"\necho \"Finished at: \"`date`\necho \"*****************************************************************\""
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/06_cellranger_count_set3.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/06_cellranger_count_set3.html",
    "title": "Generate cellranger reference for set 3",
    "section": "",
    "text": "This is for set 3\nNote that the sample IDs are in the fastq file names. The following command was used to extract them:\nfind 230327_A00748_0368_AH5CTMDSX5_fastq/ -name '*.fastq.gz' | grep -oP '(?&lt;=/)\\w{20}' | cut -c4-7 | sort | uniq  &gt; samples_set3.txt\nThere are also some Undeterimed files that contain the reads that didn’t match to any barcodes, but I manually added the missing bit of the string for that one."
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/06_cellranger_count_set3.html#note",
    "href": "04_data_analysis/001_snrnaseq_analysis/06_cellranger_count_set3.html#note",
    "title": "Generate cellranger reference for set 3",
    "section": "\n1 Note",
    "text": "1 Note\nSample P_39 gave an error, it seems the fastq files are all empty. Spoke to Jo and she can’t see anything obviously wrong on her end, so it may be that the sample was just not present due to a pipetting error perhaps. Could also be a index error which resulted in the sample data getting dumped in the “Undetermined” sample.\n\nCode#!/bin/bash\n\n#SBATCH -p c_highmem_dri1 ## dev, compute, htc, highmem\n#SBATCH --job-name=cellranger_count_set3\n#SBATCH --ntasks=40\n#SBATCH --ntasks-per-node=40\n#SBATCH --array=1-40%2\n##### #SBATCH --mem-per-cpu=8000 # memory limit per core\n#SBATCH --mem=340G # memory limit per compute node for the job\n#SBATCH --time=2-10:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%A_%a_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%A_%a_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"All jobs in this array have:\"\necho \"- SLURM_ARRAY_JOB_ID: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_COUNT: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM_ARRAY_TASK_MIN: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM_ARRAY_TASK_MAX: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM_JOB_ID: ${SLURM_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_ID: ${SLURM_ARRAY_TASK_ID}\"\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n\n## FASTQ files, set 2 with 16 samples, data from 4 runs, sequenced in Oxford\n## NOTE, V_19, only sequenced in 1 run (INPUT_DIR_1)\n## NOTE, V_20, 1 run (INPUT_DIR_2) only with a single read, skip that run\n## handled via if statement below\nINPUT_DIR=\"/gluster/dri02/rdsmbh/shared/rdsmbh/230327_A00748_0368_AH5CTMDSX5_fastq/\"\n\n## results\nOUTPUT_DIR=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/04_cellranger_count/03_set3\"\n\n## original sample IDs, correpond to FASTQ file names\nSAMPLE_ID_FILE=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/90_sample_info/samples_set3.txt\"\n\n## CR executable\nCELL_RANGER=\"/scratch/c.mpmgb/tools/cellranger-7.1.0/bin/cellranger\"\n\n## CR reference\n## pre-computed and downloaded from CR website as is (refdata-gex-GRCh38-2020-A.tar.gz)\n## based on ensembl v98/gencode v32\n## CR_REF=\"/scratch/c.mpmfw/Tools/CellRanger/CellRanger_references/refdata-gex-GRCh38-2020-A\"\n## updated CR reference based on ensembl v108 (Gencode v42)\n## from get_cellranger_reference.sh\nCR_REF=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/03_cellranger_reference/GRCh38\"\n\n\n#-----------------------------------------------------------------------\n# Cell Ranger count\n\nmkdir -p $OUTPUT_DIR\n\nN=${SLURM_ARRAY_TASK_ID}\n\nSAMPLE_ID=$(cat $SAMPLE_ID_FILE | tail -n+${N} | head -1)\n# Get the sample ID string after the first \"_\"\nSAMPLE_ID_NEW=\"${SAMPLE_ID#*_}\"\n\necho \"Input dir: \"$INPUT_DIR\necho \"Sample: \"$SAMPLE_ID\necho \"Sample new: \"$SAMPLE_ID_NEW\n\ncd $OUTPUT_DIR\n\n$CELL_RANGER count --id=$SAMPLE_ID_NEW \\\n--fastqs=$INPUT_DIR \\\n--sample=$SAMPLE_ID \\\n--transcriptome=$CR_REF \\\n--localcores=40 \\\n--localmem=330 \\\n--include-introns=true \\\n--no-bam\n\n## optional:\n## R2 length is longer (151bp) than usually recommended/required (90bp)\n## --r2-length=90\n\necho -e \"\\n*****************************************************************\"\necho \"Finished at: \"`date`\necho \"*****************************************************************\"\n\n\nOne of the many outputs of interest is the web_summary.html report files that have include various metrics worth checking. I’m not keen to add the whole output to the Git repo due to the file size, so I’ll just add these files for now.\nNote that there’s a also a web_summary.html in the SC_RNA_COUNTER_CS folder, but I don’t want that one here so I’ll also remove those.\n\nCode# get the file paths for all the \"web_summary.html\" files and save them to a file\nfind 03_data/990_processed_data/001_snrnaseq/04_cellranger_count/ -name \"web_summary.html\" &gt; web_summary_files.txt\n\n# the line length for the summary in the SC_RNA_COUNTER_CS dir is longer:\nawk '{print length}' web_summary_files.txt | sort -n | uniq -c\n# so we'll remove lines based on their length\nawk 'length &lt;= max_length' max_length=\"140\" web_summary_files.txt &gt; web_summary_files.txt\n# prepend \"git add \" to the file lines\nsed 's/^/git add /' web_summary_files_clean.txt &gt; web_summary_files_git.sh\n\n# run the file\nsh web_summary_files_git.sh\n# clean up\nrm web_summary_files.txt web_summary_files_clean.txt web_summary_files_git.sh"
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/07_cellranger_aggr.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/07_cellranger_aggr.html",
    "title": "Aggregate cellranger count reports",
    "section": "",
    "text": "cellranger count produces a report per sample, but it’s more useful to aggregate these results into one, so we have cellranger aggr for this.\nThis requires a 2 column CSV file (headers of sample_id,molecule_h5) with samples ID and the path to the molecule_info.h5 file.\nFrom the MultiQC reports, it seems the lastest batch if of better quality than the first two, so I’ll try using aggr on each set separately, and all data combined.\nYou can find these combined report for all samples here, set 1 here, set 2 here, and set 3 here.\n\nCode# list the cellranger count output directories\nfind /scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/04_cellranger_count/ -maxdepth 2 -mindepth 2 -type d &gt; cellranger_ag.txt \n\n# use the last directory in the path as the sample id\nawk -F'/' '{print $NF \",\" $0}' cellranger_ag.txt &gt; cellranger_ag2.txt\n\n# append \"outs/filtered_gene_bc_matrices\" to file paths\nsed \"s|$|/outs/molecule_info.h5|\" cellranger_ag2.txt &gt; cellranger_ag3.txt\n\n# remove the undetermined sample and P_39\ngrep -v \"Undetermined\" cellranger_ag3.txt &gt; cellranger_ag4.txt\ngrep -v \"P_39\" cellranger_ag4.txt &gt; cellranger_ag5.txt\n\n# separate out the sets\ngrep \"01_set1\" cellranger_ag5.txt &gt; cellranger_ag_set1.txt\ngrep \"02_set2\" cellranger_ag5.txt &gt; cellranger_ag_set2.txt\ngrep \"03_set3\" cellranger_ag5.txt &gt; cellranger_ag_set3.txt\n\n# add the table headers\nawk -v line=\"sample_id,molecule_h5\" 'BEGIN{print line}{print}' cellranger_ag_set1.txt &gt; cellranger_aggr_set1.csv\nawk -v line=\"sample_id,molecule_h5\" 'BEGIN{print line}{print}' cellranger_ag_set2.txt &gt; cellranger_aggr_set2.csv\nawk -v line=\"sample_id,molecule_h5\" 'BEGIN{print line}{print}' cellranger_ag_set3.txt &gt; cellranger_aggr_set3.csv\nawk -v line=\"sample_id,molecule_h5\" 'BEGIN{print line}{print}' cellranger_ag5.txt &gt; cellranger_aggr_all_sets.csv\n\n# make a file that has the file paths to use in the array job\necho \"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/05_cellranger_aggr/cellranger_aggr_set1.csv\" &gt; cellranger_aggr_csvs.txt\necho \"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/05_cellranger_aggr/cellranger_aggr_set2.csv\" &gt;&gt; cellranger_aggr_csvs.txt\necho \"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/05_cellranger_aggr/cellranger_aggr_set3.csv\" &gt;&gt; cellranger_aggr_csvs.txt\necho \"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/05_cellranger_aggr/cellranger_aggr_all_sets.csv\" &gt;&gt; cellranger_aggr_csvs.txt\n\n# clean up\nrm cellranger_ag.txt cellranger_ag2.txt cellranger_ag3.txt cellranger_ag_set1.txt cellranger_ag_set2.txt cellranger_ag_set3.txt cellranger_ag4.txt cellranger_ag5.txt\n\n\n\nCode#!/bin/bash\n\n#SBATCH -p c_vhighmem_dri1 ## dev, compute, htc, highmem\n#SBATCH --job-name=cellranger_aggr\n#SBATCH --ntasks=40\n#SBATCH --ntasks-per-node=40\n#SBATCH --array=1-4%2\n##### #SBATCH --mem-per-cpu=8000 # memory limit per core\n#SBATCH --mem=640G # memory limit per compute node for the job\n#SBATCH --time=2-10:00 # maximum job time in D-HH:MM\n#SBATCH --account=scw1329\n#SBATCH -o /scratch/c.mpmgb/hawk_output/%x_out_%A_%a_%J.txt\n#SBATCH -e /scratch/c.mpmgb/hawk_output/%x_err_%A_%a_%J.txt\n#SBATCH --mail-user Bernardo-HarringtonG@cardiff.ac.uk # email on fail\n#SBATCH --mail-type END,FAIL\n\necho \"*****************************************************************\"\necho \"All jobs in this array have:\"\necho \"- SLURM_ARRAY_JOB_ID: ${SLURM_ARRAY_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_COUNT: ${SLURM_ARRAY_TASK_COUNT}\"\necho \"- SLURM_ARRAY_TASK_MIN: ${SLURM_ARRAY_TASK_MIN}\"\necho \"- SLURM_ARRAY_TASK_MAX: ${SLURM_ARRAY_TASK_MAX}\"\necho \"This job in the array has:\"\necho \"- SLURM_JOB_ID: ${SLURM_JOB_ID}\"\necho \"- SLURM_ARRAY_TASK_ID: ${SLURM_ARRAY_TASK_ID}\"\necho \"Run on host: \"`hostname`\necho \"Number of threads (nproc): \"`nproc`\necho \"Total memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 1p`\necho \"Used memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 2p`\necho \"Free memory in GB: \"`free -g | grep -oP '\\d+' | sed -n 3p`\necho \"Username: \"`whoami`\necho \"Started at: \"`date`\necho -e \"*****************************************************************\\n\"\n\n\n## path to input csvs with samples IDs and paths to cellranger count outputs\nINPUT_CSVS=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/05_cellranger_aggr/cellranger_aggr_csvs.txt\"\n\n## results\nOUTPUT_DIR=\"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/05_cellranger_aggr\"\n\n## CR executable\nCELL_RANGER=\"/scratch/c.mpmgb/tools/cellranger-7.1.0/bin/cellranger\"\n\n\n#-----------------------------------------------------------------------\n# Cell Ranger aggr\n\nmkdir -p $OUTPUT_DIR\n\nN=${SLURM_ARRAY_TASK_ID}\n\nINPUT_CSV=$(cat $INPUT_CSVS | tail -n+${N} | head -1)\n# Subset to last part of the file path\nINPUT_CSV_NAME=\"${INPUT_CSV##*/}\"\n# Subset string to first \".\"\nSAVE_DIR=\"${INPUT_CSV_NAME%%.*}\"\n\necho \"Input CSV: \"$INPUT_CSV\necho \"Save dir: \"$SAVE_DIR\n\ncd $OUTPUT_DIR\n\n$CELL_RANGER aggr --id=$SAVE_DIR \\\n--csv=$INPUT_CSV\n\necho -e \"\\n*****************************************************************\"\necho \"Finished at: \"`date`\necho \"*****************************************************************\""
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html",
    "title": "scflow prep",
    "section": "",
    "text": "So someone in Super Computing Wales added a handy nf-core profile config we can use, and this also includes a nice guide to getting set up for using nf-core pipelines: https://github.com/nf-core/configs/blob/master/docs/scw.md\nI’ve followed the guide for offline use, which I won’t rehash here, check the above link if curious."
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html#manifest-file",
    "href": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html#manifest-file",
    "title": "scflow prep",
    "section": "\n2.1 Manifest file",
    "text": "2.1 Manifest file\nFirst let’s sort the manifest for our samples.\nHere’s how I generated it:\n\nCode# list the cellranger count output directories\nfind /scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/04_cellranger_count/ -maxdepth 2 -mindepth 2 -type d &gt; scflow_manifest.txt\n\n# use the last directory in the path as the sample id\nawk -F'/' '{print $NF \"\\t\" $0}' scflow_manifest.txt &gt; scflow_manifest2.txt\n\n# append \"outs/filtered_gene_bc_matrices\" to file paths\nsed \"s|$|/outs/filtered_feature_bc_matrix|\" scflow_manifest2.txt &gt; scflow_manifest3.txt\n\n# remove the undetermined sample and P_39\ngrep -v \"Undetermined\" scflow_manifest3.txt &gt; scflow_manifest4.txt\ngrep -v \"P_39\" scflow_manifest4.txt &gt; scflow_manifest5.txt\n\n# add the table headers\nawk -v line=\"key\\tfilepath\" 'BEGIN{print line}{print}' scflow_manifest5.txt &gt; scflow_manifest.tsv\n\n# clean up\nrm scflow_manifest.txt scflow_manifest2.txt scflow_manifest3.txt scflow_manifest4.txt scflow_manifest5.txt\n\n\nWe would also like to include the brain atlas from Yang et al. 2022.(yang2022?) Their data is on GEO so I’ve downloaded and extracted that to gluster."
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html#sample-metadata",
    "href": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html#sample-metadata",
    "title": "scflow prep",
    "section": "\n2.2 Sample metadata",
    "text": "2.2 Sample metadata\nWe also need to sort out the sample metadata for the Yang data.\n\nCodelibrary(purrr)\npkg &lt;- c(\"readr\", \"dplyr\", \"here\", \"GEOquery\", \"magrittr\", \"tidyr\", \"yaml\", \n         \"gt\")\nmap(pkg, library, character.only = TRUE)\n\n[[1]]\n[1] \"readr\"     \"purrr\"     \"stats\"     \"graphics\"  \"grDevices\" \"datasets\" \n[7] \"utils\"     \"methods\"   \"base\"     \n\n[[2]]\n [1] \"dplyr\"     \"readr\"     \"purrr\"     \"stats\"     \"graphics\"  \"grDevices\"\n [7] \"datasets\"  \"utils\"     \"methods\"   \"base\"     \n\n[[3]]\n [1] \"here\"      \"dplyr\"     \"readr\"     \"purrr\"     \"stats\"     \"graphics\" \n [7] \"grDevices\" \"datasets\"  \"utils\"     \"methods\"   \"base\"     \n\n[[4]]\n [1] \"GEOquery\"     \"Biobase\"      \"BiocGenerics\" \"here\"         \"dplyr\"       \n [6] \"readr\"        \"purrr\"        \"stats\"        \"graphics\"     \"grDevices\"   \n[11] \"datasets\"     \"utils\"        \"methods\"      \"base\"        \n\n[[5]]\n [1] \"magrittr\"     \"GEOquery\"     \"Biobase\"      \"BiocGenerics\" \"here\"        \n [6] \"dplyr\"        \"readr\"        \"purrr\"        \"stats\"        \"graphics\"    \n[11] \"grDevices\"    \"datasets\"     \"utils\"        \"methods\"      \"base\"        \n\n[[6]]\n [1] \"tidyr\"        \"magrittr\"     \"GEOquery\"     \"Biobase\"      \"BiocGenerics\"\n [6] \"here\"         \"dplyr\"        \"readr\"        \"purrr\"        \"stats\"       \n[11] \"graphics\"     \"grDevices\"    \"datasets\"     \"utils\"        \"methods\"     \n[16] \"base\"        \n\n[[7]]\n [1] \"yaml\"         \"tidyr\"        \"magrittr\"     \"GEOquery\"     \"Biobase\"     \n [6] \"BiocGenerics\" \"here\"         \"dplyr\"        \"readr\"        \"purrr\"       \n[11] \"stats\"        \"graphics\"     \"grDevices\"    \"datasets\"     \"utils\"       \n[16] \"methods\"      \"base\"        \n\n[[8]]\n [1] \"gt\"           \"yaml\"         \"tidyr\"        \"magrittr\"     \"GEOquery\"    \n [6] \"Biobase\"      \"BiocGenerics\" \"here\"         \"dplyr\"        \"readr\"       \n[11] \"purrr\"        \"stats\"        \"graphics\"     \"grDevices\"    \"datasets\"    \n[16] \"utils\"        \"methods\"      \"base\"        \n\n\n\nCode# current metadata structure for first 2 sample batches\nmetadata &lt;- read_tsv(here(\"03_data/990_processed_data/001_snrnaseq/90_sample_info/samples_meta_merged_final.txt\"))\nnames(metadata)\n# add a column for the technique - don't know much about it so I'll just call\n# it Oxford for now\nmetadata$technique &lt;- \"oxford\"\n# also don't know brain region\nmetadata$brain_region &lt;- \"dunno\"\n\n# get yang data\ngsm &lt;- getGEO(\"GSE163577\")\n# get phenotype data\nphenodata &lt;- gsm$GSE163577_series_matrix.txt.gz@phenoData@data\n# select useful cols\nphenodata &lt;- phenodata %&gt;%\n  dplyr::select(\n    title,\n    geo_accession,\n    extract_protocol_ch1,\n    data_processing,\n    platform_id,\n    instrument_model,\n    supplementary_file_1,\n    `brain region:ch1`,\n    `disease:ch1`\n  )\n\n# load scflow test sample sheet\nexample_samplesheet &lt;- read_tsv(here(\"03_data/990_processed_data/001_snrnaseq/90_sample_info/scflowSampleSheet_example.tsv\"))\n\n\nThere are nice geo accessions that would be handy to use as unique sample IDs for the yang manifest file\n\nCode# save id and path for manifest file\nyang_manifest &lt;- phenodata %&gt;%\n  dplyr::select(geo_accession, supplementary_file_1)\n\n# get the last part of path\nlast_path &lt;- sub(\".*/\", \"\", yang_manifest$supplementary_file_1)\n# get after first _\nlast_path &lt;- sub(\"^[^_]*_\", \"\", last_path)\n# remove file extension\nlast_path &lt;- gsub(\"\\\\.tar\\\\.gz$\", \"\", last_path)\n\n# update path to yang data on gluster\nyang_manifest$supplementary_file_1 &lt;-\n  paste0(\n    \"/scratch/scw1329/gmbh/blood-brain-barrier-in-ad/03_data/990_processed_data/001_snrnaseq/06_scflow/yang_2022_data/\",\n    last_path\n  )\n# update names to be in line with scflow manifest file\nnames(yang_manifest) &lt;- c(\"key\", \"filepath\")\n# save to tsv\nwrite_tsv(yang_manifest, here(\"03_data/990_processed_data/001_snrnaseq/06_scflow/yang_manifest.tsv\"))\n\n\nThen we just need to merge the two manifest files together.\n\nCode(cat scflow_manifest.tsv && tail -n +2 yang_manifest.tsv) &gt; scflow_manifest_both.tsv\n\n\nIt turns out that you can’t have delimiters in the key for the mainfest/sample files, so I need to remove the underscores too\n\nCodeawk 'BEGIN{FS=OFS=\"\\t\"} {gsub(\"_\", \"\", $1)} 1'  scflow_manifest_both.tsv &gt; scflow_manifest_final.tsv\n\n\nWe need to get the phenotype data from the yang GEO data in line with our phenotype data to join them. My understanding from the Yang paper is that they only looked at the vascular/perivascular component, so I’ll label the prep as V for now.\n\nCode# sort yang phenotype data and join to ours\nmerged_meta &lt;- phenodata %&gt;%\n  dplyr::rename(sample = geo_accession,\n                brain_region = `brain region:ch1`,\n                type = `disease:ch1`) %&gt;%\n  # add cell subsetting technique and cell population\n  dplyr::mutate(technique = \"VINE-seq\", prep = \"V\") %&gt;%\n  # select cols of interest\n  dplyr::select(sample, brain_region, type, technique, prep) %&gt;%\n  # merge datasets\n  dplyr::bind_rows(metadata, .) %&gt;%\n  # make control label consistent\n  dplyr::mutate(type = ifelse(type == \"CT\", \"Control\", type)) %&gt;%\n  # add final group column for differential expression\n  dplyr::mutate(group = paste(type, prep, technique, brain_region, sep = \"_\"),\n                sample = gsub(\"_\", \"\", sample)) %&gt;%\n  # rename to sample sheet style\n  dplyr::rename(manifest = sample,\n                diagnosis = type)\n\n\n\nCode# save metadata\nwrite_tsv(merged_meta, file = here(\"03_data/990_processed_data/001_snrnaseq/90_sample_info/sample_metadata.tsv\"))"
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html#getting-marker-genes",
    "href": "04_data_analysis/001_snrnaseq_analysis/08_scflow_prep.html#getting-marker-genes",
    "title": "scflow prep",
    "section": "\n2.3 Getting marker genes",
    "text": "2.3 Getting marker genes\nThe EBI has the Single cell expression atlas so I’ll use that for our gene annotation markers. There is an R package on Bioconductor for querying it, but it doesn’t seem to have an easy way of querying by tissues/organ, so I’ll just manually download them (there aren’t that many).\n\nCode# list gene marker csvs\ngene_markers_files &lt;-\n  list.files(\n    here(\n      \"03_data/990_processed_data/001_snrnaseq/06_scflow/2023-05-22_gene-markers\"\n    ),\n    full.names = TRUE\n  )\n\n# Extract the final part of the path after the last \"/\"\nfile_names &lt;- basename(gene_markers_files)\n# Remove the file extension\nfile_names &lt;- tools::file_path_sans_ext(file_names)\n\ngene_markers &lt;- map(gene_markers_files, read_csv) %&gt;%\n  set_names(file_names)\n\n# add annotation name\ngene_markers &lt;-\n  map2(gene_markers, file_names, ~ dplyr::mutate(.x, annotation = .y)) %&gt;%\n  # bind rows\n  dplyr::bind_rows()\n\n# the odd column headers are the experiment ID and the values are expression\n# pivot long\ngene_markers &lt;- gene_markers %&gt;%\n  pivot_longer(!c(Category, annotation),\n               names_to = \"experiment_id\",\n               values_to = \"expression\") %&gt;%\n  # remove cases where expression is 0\n  dplyr::filter(!is.na(expression)) %&gt;%\n  dplyr::rename(gene = Category)\n\n# Filter out duplicate genes within each annotation\ngene_markers_clean &lt;- gene_markers %&gt;%\n  group_by(annotation) %&gt;%\n  filter(!duplicated(gene)) %&gt;%\n  # remove the \"top-genes\" part of the string\n  dplyr::mutate(annotation = gsub(\"top-.*\", \"\", annotation))\n\ngt(gene_markers_clean)\n\n\n\n\n\n\ngene\n      experiment_id\n      expression\n    \n\n\nastrocyte-of-the-cerebra\n    \n\nCLU\nE-HCAD-35\n5733.4755\n\n\nPTGDS\nE-HCAD-35\n2628.1210\n\n\nSLC1A2\nE-HCAD-35\n4977.4883\n\n\nSLC1A3\nE-HCAD-35\n4415.0110\n\n\nMT2A\nE-HCAD-35\n2587.3220\n\n\nastrocyte\n    \n\nSPARCL1\nE-GEOD-84465\n12453.4555\n\n\nATP1A2\nE-GEOD-84465\n2682.5862\n\n\nCPE\nE-GEOD-84465\n6516.6291\n\n\nNTRK2\nE-GEOD-84465\n2098.7989\n\n\nSLC1A2\nE-GEOD-84465\n6660.5045\n\n\nb-cell\n    \n\nRPS19\nE-HCAD-35\n5104.1410\n\n\nRPL36\nE-HCAD-35\n2518.1768\n\n\nRPS28\nE-HCAD-35\n5210.8823\n\n\nRPLP1\nE-HCAD-35\n5189.7509\n\n\nENSG00000223532\nE-HCAD-35\n1006.6358\n\n\ncerebral-cortex-endothel\n    \n\nCLDN5\nE-HCAD-35\n3000.6105\n\n\nIFITM3\nE-HCAD-35\n3683.9235\n\n\nTMSB10\nE-HCAD-35\n8179.4912\n\n\nMT2A\nE-HCAD-35\n6327.5722\n\n\nRPS28\nE-HCAD-35\n3475.7807\n\n\ncerebral-cortex-glial-ce\n    \n\nPLP1\nE-HCAD-35\n4761.9043\n\n\nMBP\nE-HCAD-35\n2057.6130\n\n\nDST\nE-HCAD-35\n2372.8813\n\n\nPTGDS\nE-HCAD-35\n1721.1704\n\n\nPCDH9\nE-HCAD-35\n2728.5132\n\n\ncortical-interneurontop\n    \n\nTCF4\nE-HCAD-35\n1566.5796\n\n\nGAD1\nE-HCAD-35\n735.2941\n\n\nATP1B1\nE-HCAD-35\n2055.8003\n\n\nERBB4\nE-HCAD-35\n556.0189\n\n\nSLC6A1\nE-HCAD-35\n641.4368\n\n\ncortical-layer-2-3-excit\n    \n\nPPFIA2\nE-HCAD-35\n441.5986\n\n\nENC1\nE-HCAD-35\n1096.4912\n\n\nR3HDM1\nE-HCAD-35\n403.3885\n\n\nCAMK2A\nE-HCAD-35\n883.3923\n\n\nGRIA2\nE-HCAD-35\n1666.6666\n\n\ncortical-layer-4-excitat\n    \n\nMEF2C\nE-HCAD-35\n1343.1840\n\n\nRORB\nE-HCAD-35\n757.6046\n\n\nSYT1\nE-HCAD-35\n3600.6842\n\n\nIPCEF1\nE-HCAD-35\n252.1731\n\n\nNRGN\nE-HCAD-35\n740.1240\n\n\ncortical-layer-5-6-excit\n    \n\nNPTX1\nE-HCAD-35\n553.7098\n\n\nCADPS\nE-HCAD-35\n447.4273\n\n\nCELF2\nE-HCAD-35\n1155.2681\n\n\nSLC17A7\nE-HCAD-35\n440.8200\n\n\nARPP21\nE-HCAD-35\n418.7605\n\n\nexcitatory-neuron\n    \n\nCHN1\nE-HCAD-35\n2145.9229\n\n\nNRGN\nE-HCAD-35\n1000.0001\n\n\nPPP3CA\nE-HCAD-35\n1136.3636\n\n\nSYT1\nE-HCAD-35\n3023.4316\n\n\nMT-ND3\nE-HCAD-35\n2506.2656\n\n\nimmune-cell\n    \n\nCD74\nE-GEOD-84465\n9703.2005\n\n\nTYROBP\nE-GEOD-84465\n585.7133\n\n\nFCER1G\nE-GEOD-84465\n791.5301\n\n\nSRGN\nE-GEOD-84465\n1105.6933\n\n\nLAPTM5\nE-GEOD-84465\n1690.8645\n\n\nmicroglial-cell\n    \n\nFTL\nE-HCAD-35\n10436.6890\n\n\nCD74\nE-HCAD-35\n3796.1101\n\n\nTMSB4X\nE-HCAD-35\n7035.1340\n\n\nSPP1\nE-HCAD-35\n8000.5120\n\n\nRPL28\nE-HCAD-35\n2514.1431\n\n\nneoplastic-cell\n    \n\nEGFR\nE-GEOD-84465\n704.7337\n\n\nCLU\nE-GEOD-84465\n4150.2519\n\n\nCNN3\nE-GEOD-84465\n1001.4115\n\n\nSEC61G\nE-GEOD-84465\n162.9971\n\n\nTUBA1A\nE-GEOD-84465\n2820.4468\n\n\nneuron\n    \n\nGAD1\nE-GEOD-84465\n847.4346\n\n\nTMEM130\nE-GEOD-84465\n1164.7417\n\n\nSNAP25\nE-GEOD-84465\n2454.8066\n\n\nTUBA4A\nE-GEOD-84465\n308.4678\n\n\nGAD2\nE-GEOD-84465\n497.7964\n\n\noligodendrocyte-precurso\n    \n\nSCD5\nE-GEOD-84465\n1691.6135\n\n\nPLLP\nE-GEOD-84465\n1079.2020\n\n\nVCAN\nE-HCAD-35\n1857.0101\n\n\nPTPRZ1\nE-HCAD-35\n3211.9915\n\n\nAPOD\nE-HCAD-35\n1618.1230\n\n\nTNR\nE-HCAD-35\n1216.5449\n\n\nOLIG1\nE-HCAD-35\n1176.4706\n\n\noligodendrocyte\n    \n\nDBNDD2\nE-GEOD-84465\n5190.3916\n\n\nQDPR\nE-GEOD-84465\n4438.4138\n\n\nCRYAB\nE-GEOD-84465\n4762.0579\n\n\nGPM6B\nE-HCAD-35\n3275.1094\n\n\nTMEM144\nE-HCAD-35\n2217.2950\n\n\nMBP\nE-HCAD-35\n4975.1240\n\n\nTF\nE-HCAD-35\n3060.4436\n\n\nPLP1\nE-HCAD-35\n11936.3400\n\n\nphagocyte\n    \n\nSPP1\nE-HCAD-35\n4012.8408\n\n\nPLP1\nE-HCAD-35\n6361.3230\n\n\nMBP\nE-HCAD-35\n2223.8696\n\n\nSLC1A3\nE-HCAD-35\n1540.8320\n\n\nFTL\nE-HCAD-35\n2322.8804\n\n\npyramidal-neuron\n    \n\nMAP1B\nE-HCAD-35\n4305.0430\n\n\nCALM2\nE-HCAD-35\n3703.7036\n\n\nTUBA1B\nE-HCAD-35\n2306.8050\n\n\nCALM1\nE-HCAD-35\n3956.4788\n\n\nCHN1\nE-HCAD-35\n4028.1975\n\n\nstromal-cell\n    \n\nIFITM3\nE-HCAD-35\n2531.6458\n\n\nSPARC\nE-HCAD-35\n1148.1056\n\n\nPDGFRB\nE-HCAD-35\n985.2216\n\n\nIFITM1\nE-HCAD-35\n1166.8611\n\n\nDCN\nE-HCAD-35\n1043.8413\n\n\nt-cell\n    \n\nRPS28\nE-HCAD-35\n1914.8449\n\n\nRPS8\nE-HCAD-35\n1225.6691\n\n\nRPL41\nE-HCAD-35\n2276.2886\n\n\nENSG00000223367\nE-HCAD-35\n422.7055\n\n\nRPLP2\nE-HCAD-35\n1254.5843\n\n\nvascular-cell\n    \n\nIGFBP7\nE-GEOD-84465\n2707.6829\n\n\nIFITM1\nE-GEOD-84465\n951.4264\n\n\nIFITM3\nE-GEOD-84465\n1115.2355\n\n\nSPARC\nE-GEOD-84465\n3253.8726\n\n\nGNG11\nE-GEOD-84465\n235.9478\n\n\n\n\n\n\nNow we can generate the yaml file we need from this dataframe\n\nCode# Group the dataframe by 'annotation' and aggregate genes as a list\nresult &lt;- gene_markers_clean %&gt;%\n  group_by(annotation) %&gt;%\n  summarise(genes = list(unique(gene))) %&gt;%\n  ungroup()\n\n# Convert the result to a nested list using map()\nresult_list &lt;- map(set_names(result$genes, result$annotation), as.list)\n\n# Write the YAML to a file\nyaml_output &lt;- write_yaml(\n  result_list,\n  here(\n    \"03_data/990_processed_data/001_snrnaseq/06_scflow/reddim_genes.yml\"\n  )\n)"
  },
  {
    "objectID": "04_data_analysis/001_snrnaseq_analysis/09_scflow_run.html",
    "href": "04_data_analysis/001_snrnaseq_analysis/09_scflow_run.html",
    "title": "Run scflow",
    "section": "",
    "text": "1 Custom config file\nNote that by default the scw config uses the htc queue and we’d rather use our DRI nodes, so I’ll overwrite that.\nI’m also trying out Nextflow Tower for the first time, it should be nice for keeping an eye on runs, seeing resource usage and troubleshooting any issues. Note that after I purl this config I’ll manually update the Nextflow Tower token on hawk.\n\nCodeexecutor { \n  name = 'slurm' \n  queueSize = 10\n  queue = 'c_vhighmem_dri1'\n}\n\nparams {\n  max_memory = 700.GB\n  max_cpus = 30\n  max_time = 82.h\n}\n\ntower {\n  accessToken = keyring::key_get(\"hawk_nextflow_tower\")\n  enabled = true\n}\n\nprocess {\n    clusterOptions = '--account=scw1329'\n    beforeScript = 'module load singularity'\n    withName: SCFLOW_MERGE {\n      memory = 300.GB\n    }\n    withName: SCFLOW_INTEGRATE {\n      memory = 500.GB\n      queue = 'c_vhighmem_dri1'\n    }\n}\n\n\nNote that I’m using the dev-nf branch to run.\nThere’s two nextflow run calls, the first is with both our data and the Yang data, and the latter with just our data in case something goes wrong with that. (Yang et al. 2022)\n\nCodenextflow run combiz-nf-core-scflow-dev-nf/workflow/ -profile scw -c 03_data/990_processed_data/001_snrnaseq/06_scflow/pipeline.config --manifest 03_data/990_processed_data/001_snrnaseq/06_scflow/scflow_manifest_final.tsv --input 03_data/990_processed_data/001_snrnaseq/90_sample_info/sample_metadata.tsv --reddim_genes_yml 03_data/990_processed_data/001_snrnaseq/06_scflow/reddim_genes.yml --ensembl_mappings 03_data/990_processed_data/001_snrnaseq/06_scflow/ensembl_mappings.tsv --email Bernardo-HarringtonG@cardiff.ac.uk -resume -bg &gt; 03_data/990_processed_data/001_snrnaseq/06_scflow/scflow.log\n# version with our samples alone\nnextflow run combiz-nf-core-scflow-dev-nf/workflow/ -profile scw -c 03_data/990_processed_data/001_snrnaseq/06_scflow/pipeline.config --manifest 03_data/990_processed_data/001_snrnaseq/06_scflow/scflow_manifest.tsv --input 03_data/990_processed_data/001_snrnaseq/90_sample_info/sample_metadata_ours.tsv --reddim_genes_yml 03_data/990_processed_data/001_snrnaseq/06_scflow/reddim_genes.yml --ensembl_mappings 03_data/990_processed_data/001_snrnaseq/06_scflow/ensembl_mappings.tsv --email Bernardo-HarringtonG@cardiff.ac.uk -resume -bg &gt; 03_data/990_processed_data/001_snrnaseq/06_scflow/scflow.log\n\n\n\n\n\n\n\nReferences\n\nYang, Andrew C., Ryan T. Vest, Fabian Kern, Davis P. Lee, Maayan Agam, Christina A. Maat, Patricia M. Losada, et al. 2022. “A Human Brain Vascular Atlas Reveals Diverse Mediators of Alzheimer’s Risk.” Nature 603 (7903): 885–92. https://doi.org/10.1038/s41586-021-04369-3."
  },
  {
    "objectID": "04_data_analysis/990_code_libraries/01_r-packages-used.html",
    "href": "04_data_analysis/990_code_libraries/01_r-packages-used.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView SourceHere is we install/load all the packages we use in this project.\n\nCode# create library calls for renv\nlibrary_calls &lt;- paste0(\"library(\", c(pkg, pkg_bioconductor), \")\")\nwrite_lines(library_calls, here(\"04_data_analysis/990_code_libraries/02_library-calls-for-renv.R\"))\n\n\n\nCoderm(pkg, pkg_bioconductor, sort_packages)"
  },
  {
    "objectID": "06_dissemination/03_website/01_data-analysis.html",
    "href": "06_dissemination/03_website/01_data-analysis.html",
    "title": "Data Analysis Pipeline",
    "section": "",
    "text": "This section of the site houses the data analysis performed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BBB in AD",
    "section": "",
    "text": "This is a Quarto website that outlines the analysis pipeline of this project."
  },
  {
    "objectID": "index.html#renv",
    "href": "index.html#renv",
    "title": "BBB in AD",
    "section": "\n1.1 renv",
    "text": "1.1 renv\nNote that the project used renv to manage the R packages used. When you first clone the repo to your local machine, you’ll need to run renv::restore() in R for it to establish all the packages needed."
  },
  {
    "objectID": "index.html#website-execution-caching",
    "href": "index.html#website-execution-caching",
    "title": "BBB in AD",
    "section": "\n1.2 Website execution caching",
    "text": "1.2 Website execution caching\nNote that the website uses caching to save the results of the scripts to the _freeze directory.\nThis is helpful as it allows the website to be built quickly after the first build, but it can be tricky as it will only rerun a given script if it sees a change in that script. So, if you change an earlier script that would alter the inputs of later scripts, it wouldn’t rerun those script as the code hadn’t changed, and so they wouldn’t change on the website.\nIf you run into any issues with this, just delete the _freeze directory and build the website again."
  }
]